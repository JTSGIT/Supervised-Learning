{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The os module has a perfect method to list files in a directory.\n",
    "- Pandas json normalize could work here but is not necessary to convert the JSON data to a dataframe.\n",
    "- You may need a nested for-loop to access each sale!\n",
    "- We've put a lot of time into creating the structure of this repository, and it's a good example for future projects.  In the file functions_variables.py, there is an example function that you can import and use.  If you have any variables, functions or classes that you want to make, they can be put in the functions_variables.py file and imported into a notebook.  Note that only .py files can be imported into a notebook. If you want to import everything from a .py file, you can use the following:\n",
    "```python\n",
    "from functions_variables import *\n",
    "```\n",
    "If you just import functions_variables, then each object from the file will need to be prepended with \"functions_variables\"\\\n",
    "Using this .py file will keep your notebooks very organized and make it easier to reuse code between notebooks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:16:15.065010Z",
     "start_time": "2024-05-21T00:16:15.062086Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# (this is not an exhaustive list of libraries)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from functions_variables import encode_tags"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load one file first to see what type of data you're dealing with and what attributes it has"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:18:28.530288Z",
     "start_time": "2024-05-21T00:18:28.526880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory you want to explore\n",
    "directory = '/Users/jorgen/Documents/LHL/Supervised-Learning-main/data'\n",
    "\n",
    "# Use os.listdir to get files\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Print all files\n",
    "for file in files:\n",
    "    print(file)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA_Harrisburg_0.json\n",
      "LA_BatonRouge_2.json\n",
      "MO_JeffersonCity_0.json\n",
      "TN_Nashville_2.json\n",
      "MD_Annapolis_4.json\n",
      "NM_SantaFe_3.json\n",
      "AZ_Phoenix_0.json\n",
      "KY_Frankfort_4.json\n",
      "CO_Denver_1.json\n",
      "FL_Tallahassee_3.json\n",
      "MI_Lansing_2.json\n",
      "AR_LittleRock_0.json\n",
      "ME_Augusta_4.json\n",
      "GA_Atlanta_4.json\n",
      "IN_Indianapolis_0.json\n",
      "MN_St.Paul_4.json\n",
      "MA_Boston_1.json\n",
      "WI_Madison_1.json\n",
      "NE_Lincoln_4.json\n",
      "AL_Montgomery_4.json\n",
      "CA_Sacramento_1.json\n",
      "IA_DesMoines_1.json\n",
      "AK_Juneau_0.json\n",
      "OK_OklahomaCity_0.json\n",
      "NC_Raleigh_0.json\n",
      "NH_Concord_0.json\n",
      "DE_Dover_1.json\n",
      "MT_Helena_1.json\n",
      "MS_Jackson_0.json\n",
      "MT_Helena_0.json\n",
      "MS_Jackson_1.json\n",
      "RI_Providence_4.json\n",
      "NH_Concord_1.json\n",
      "CT_Hartford_4.json\n",
      "DE_Dover_0.json\n",
      "NC_Raleigh_1.json\n",
      "AK_Juneau_1.json\n",
      "OK_OklahomaCity_1.json\n",
      "IA_DesMoines_0.json\n",
      "NY_Albany_4.json\n",
      "WY_Cheyenne_4.json\n",
      "CA_Sacramento_0.json\n",
      "WI_Madison_0.json\n",
      "MA_Boston_0.json\n",
      "IL_Springfield_4.json\n",
      "IN_Indianapolis_1.json\n",
      "AR_LittleRock_1.json\n",
      "VT_Montpelier_4.json\n",
      "CO_Denver_0.json\n",
      "FL_Tallahassee_2.json\n",
      "MI_Lansing_3.json\n",
      "WA_Olympia_4.json\n",
      "UT_SaltLakeCity_4.json\n",
      "AZ_Phoenix_1.json\n",
      "NM_SantaFe_2.json\n",
      "MO_JeffersonCity_1.json\n",
      "TN_Nashville_3.json\n",
      "LA_BatonRouge_3.json\n",
      "PA_Harrisburg_1.json\n",
      "IL_Springfield_3.json\n",
      "MN_St.Paul_2.json\n",
      "NE_Lincoln_2.json\n",
      "AL_Montgomery_2.json\n",
      "SC_Columbia_0.json\n",
      "WY_Cheyenne_3.json\n",
      "NY_Albany_3.json\n",
      "WV_Charleston_1.json\n",
      "TX_Austin_0.json\n",
      "OH_Columbus_0.json\n",
      "CT_Hartford_3.json\n",
      "OR_Salem_0.json\n",
      "RI_Providence_3.json\n",
      "SD_Pierre_0.json\n",
      "KS_Topeka_1.json\n",
      "LA_BatonRouge_4.json\n",
      "TN_Nashville_4.json\n",
      "MD_Annapolis_2.json\n",
      "NV_CarsonCity_1.json\n",
      "HI_Honolulu_1.json\n",
      "VA_Richmond_1.json\n",
      ".gitkeep\n",
      "ND_Bismarck_0.json\n",
      "KY_Frankfort_2.json\n",
      "UT_SaltLakeCity_3.json\n",
      "NJ_Trenton_1.json\n",
      "MI_Lansing_4.json\n",
      "WA_Olympia_3.json\n",
      "GA_Atlanta_2.json\n",
      "ME_Augusta_2.json\n",
      "VT_Montpelier_3.json\n",
      "ID_Boise_0.json\n",
      "ID_Boise_1.json\n",
      "GA_Atlanta_3.json\n",
      "ME_Augusta_3.json\n",
      "VT_Montpelier_2.json\n",
      "FL_Tallahassee_4.json\n",
      "WA_Olympia_2.json\n",
      "KY_Frankfort_3.json\n",
      "UT_SaltLakeCity_2.json\n",
      "NJ_Trenton_0.json\n",
      "VA_Richmond_0.json\n",
      "ND_Bismarck_1.json\n",
      "NM_SantaFe_4.json\n",
      "NV_CarsonCity_0.json\n",
      "HI_Honolulu_0.json\n",
      "MD_Annapolis_3.json\n",
      "KS_Topeka_0.json\n",
      "SD_Pierre_1.json\n",
      "RI_Providence_2.json\n",
      "OH_Columbus_1.json\n",
      "CT_Hartford_2.json\n",
      "OR_Salem_1.json\n",
      "TX_Austin_1.json\n",
      "WV_Charleston_0.json\n",
      "WY_Cheyenne_2.json\n",
      "NY_Albany_2.json\n",
      "SC_Columbia_1.json\n",
      "AL_Montgomery_3.json\n",
      "NE_Lincoln_3.json\n",
      "MN_St.Paul_3.json\n",
      "IL_Springfield_2.json\n",
      "SD_Pierre_2.json\n",
      "MS_Jackson_4.json\n",
      "RI_Providence_1.json\n",
      "OR_Salem_2.json\n",
      "CT_Hartford_1.json\n",
      "NH_Concord_4.json\n",
      "OH_Columbus_2.json\n",
      "NC_Raleigh_4.json\n",
      "TX_Austin_2.json\n",
      "OK_OklahomaCity_4.json\n",
      "AK_Juneau_4.json\n",
      "WV_Charleston_3.json\n",
      "NY_Albany_1.json\n",
      "WY_Cheyenne_1.json\n",
      "SC_Columbia_2.json\n",
      "AL_Montgomery_0.json\n",
      "NE_Lincoln_0.json\n",
      "MN_St.Paul_0.json\n",
      "IL_Springfield_1.json\n",
      "IN_Indianapolis_4.json\n",
      "ID_Boise_2.json\n",
      "AR_LittleRock_4.json\n",
      "VT_Montpelier_1.json\n",
      "GA_Atlanta_0.json\n",
      "ME_Augusta_0.json\n",
      "WA_Olympia_1.json\n",
      "NJ_Trenton_3.json\n",
      "UT_SaltLakeCity_1.json\n",
      "KY_Frankfort_0.json\n",
      "ND_Bismarck_2.json\n",
      "AZ_Phoenix_4.json\n",
      "VA_Richmond_3.json\n",
      "HI_Honolulu_3.json\n",
      "NV_CarsonCity_3.json\n",
      "MD_Annapolis_0.json\n",
      "MO_JeffersonCity_4.json\n",
      "PA_Harrisburg_4.json\n",
      "KS_Topeka_3.json\n",
      "KS_Topeka_2.json\n",
      "MD_Annapolis_1.json\n",
      "HI_Honolulu_2.json\n",
      "NV_CarsonCity_2.json\n",
      "ND_Bismarck_3.json\n",
      "VA_Richmond_2.json\n",
      "NJ_Trenton_2.json\n",
      "UT_SaltLakeCity_0.json\n",
      "KY_Frankfort_1.json\n",
      "WA_Olympia_0.json\n",
      "CO_Denver_4.json\n",
      "VT_Montpelier_0.json\n",
      "GA_Atlanta_1.json\n",
      "ME_Augusta_1.json\n",
      "ID_Boise_3.json\n",
      "IL_Springfield_0.json\n",
      "MN_St.Paul_1.json\n",
      "MA_Boston_4.json\n",
      "WI_Madison_4.json\n",
      "NE_Lincoln_1.json\n",
      "AL_Montgomery_1.json\n",
      "SC_Columbia_3.json\n",
      "CA_Sacramento_4.json\n",
      "NY_Albany_0.json\n",
      "WY_Cheyenne_0.json\n",
      "IA_DesMoines_4.json\n",
      "WV_Charleston_2.json\n",
      "TX_Austin_3.json\n",
      "OR_Salem_3.json\n",
      "CT_Hartford_0.json\n",
      "OH_Columbus_3.json\n",
      "DE_Dover_4.json\n",
      "RI_Providence_0.json\n",
      "MT_Helena_4.json\n",
      "SD_Pierre_3.json\n",
      "IN_Indianapolis_2.json\n",
      "ID_Boise_4.json\n",
      "AR_LittleRock_2.json\n",
      "FL_Tallahassee_1.json\n",
      "MI_Lansing_0.json\n",
      "CO_Denver_3.json\n",
      "ND_Bismarck_4.json\n",
      "AZ_Phoenix_2.json\n",
      "NM_SantaFe_1.json\n",
      "TN_Nashville_0.json\n",
      "MO_JeffersonCity_2.json\n",
      "LA_BatonRouge_0.json\n",
      "PA_Harrisburg_2.json\n",
      "SD_Pierre_4.json\n",
      "MS_Jackson_2.json\n",
      "MT_Helena_3.json\n",
      "OR_Salem_4.json\n",
      "DE_Dover_3.json\n",
      "OH_Columbus_4.json\n",
      "NH_Concord_2.json\n",
      "NC_Raleigh_2.json\n",
      "TX_Austin_4.json\n",
      "AK_Juneau_2.json\n",
      "OK_OklahomaCity_2.json\n",
      "IA_DesMoines_3.json\n",
      "CA_Sacramento_3.json\n",
      "SC_Columbia_4.json\n",
      "WI_Madison_3.json\n",
      "MA_Boston_3.json\n",
      "processed\n",
      "MA_Boston_2.json\n",
      "WI_Madison_2.json\n",
      "CA_Sacramento_2.json\n",
      "IA_DesMoines_2.json\n",
      "WV_Charleston_4.json\n",
      "AK_Juneau_3.json\n",
      "OK_OklahomaCity_3.json\n",
      "NC_Raleigh_3.json\n",
      "DE_Dover_2.json\n",
      "NH_Concord_3.json\n",
      "MS_Jackson_3.json\n",
      "MT_Helena_2.json\n",
      "KS_Topeka_4.json\n",
      "PA_Harrisburg_3.json\n",
      "LA_BatonRouge_1.json\n",
      "TN_Nashville_1.json\n",
      "MO_JeffersonCity_3.json\n",
      "HI_Honolulu_4.json\n",
      "NV_CarsonCity_4.json\n",
      "NM_SantaFe_0.json\n",
      "AZ_Phoenix_3.json\n",
      "VA_Richmond_4.json\n",
      "NJ_Trenton_4.json\n",
      "FL_Tallahassee_0.json\n",
      "MI_Lansing_1.json\n",
      "CO_Denver_2.json\n",
      "AR_LittleRock_3.json\n",
      "IN_Indianapolis_3.json\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, ensure that you have all sales in a dataframe.\n",
    "- Is each cell one value, or do some cells have lists?\n",
    "- Maybe the \"tags\" will help create some features.\n",
    "- What are the data types of each column?\n",
    "- Some sales may not actually include the sale price.  These rows should be dropped.\n",
    "- Some sales don't include the property type.\n",
    "- There are a lot of None values.  Should these be dropped or replaced with something?"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:50:58.442677Z",
     "start_time": "2024-05-21T00:50:58.403288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Analyze each cell in the DataFrame to check if they contain lists\n",
    "for column in X_train.columns:\n",
    "    has_list = X_train[column].apply(lambda x: isinstance(x, list)).any()\n",
    "    if has_list:\n",
    "        print(f\"Column '{column}' contains a list\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'tags' contains a list\n",
      "Column 'branding' contains a list\n",
      "Column 'photos' contains a list\n",
      "Column 'virtual_tours' contains a list\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:51:10.641439Z",
     "start_time": "2024-05-21T00:51:10.632433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If a 'tags' column exists, try printing out unique set of tags\n",
    "if 'tags' in X_train.columns:\n",
    "    unique_tags = set(tag for tags in X_train['tags'].dropna() for tag in tags)\n",
    "    print(f\"Unique tags: {unique_tags}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tags: {'security', 'courtyard_entry', 'clubhouse', 'river_view', 'solar_panels', 'golf_course_lot_or_frontage', 'lake_view', 'hardwood_floors', 'large_kitchen', 'energy_efficient', 'horse_property', 'no_hoa', 'large_porch', 'fireplace', 'den_or_office', 'open_house', 'community_center', 'handicap_access', 'marina', 'community_tennis_court', 'soccer', 'low_hoa', 'shopping', 'community_horse_facilities', 'master_bedroom', 'spa_or_hot_tub', 'greenbelt', 'screen_porch', 'water_view', 'high_ceiling', 'big_lot', 'maintenance', 'equestrian', 'first_floor_master_bedroom', 'farm', 'fenced_yard', 'community_clubhouse', 'ranch', 'medicalcare', 'pets_allowed', 'senior_community', 'kitchen_island', 'guest_house', 'investment_opportunity', 'groundscare', 'white_kitchen', 'guest_parking', 'outbuilding', 'lake', 'mountain_view', 'two_kitchen', 'forced_air', 'park', 'basketball', 'garage_3_or_more', 'ocean_view', 'front_porch', 'indoor_basketball_court', 'gated_community', 'library', 'detached_guest_house', 'vaulted_ceiling', 'beach', 'volleyball', 'golf_course_view', 'storm_shelter', 'smart_homes', 'outdoor_kitchen', 'fenced_courtyard', 'recreation_facilities', 'open_kitchen', 'private_courtyard', 'garage_2_or_more', 'big_bathroom', 'elevator', 'laundry_room', 'waterfront', 'wine_cellar', 'basement', 'big_yard', 'rv_or_boat_parking', 'community_swimming_pool', 'community_boat_facilities', 'community_gym', 'tennis_court', 'rental_property', 'wooded_land', 'horse_stables', 'central_air', 'dishwasher', 'golf_course', 'pond', 'corner_lot', 'private_backyard', 'master_suite', 'garage_1_or_more', 'community_park', 'views', 'dual_master_bedroom', 'tennis', 'jack_and_jill_bathroom', 'fixer_upper', 'wrap_around_porch', 'dining_room', 'trails', 'granite_kitchen', 'game_room', 'single_story', 'private_parking', 'hill_or_mountain_view', 'solar_system', 'community_spa_or_hot_tub', 'horse_facilities', 'master_bathroom', 'cul_de_sac', 'furniture', 'community_outdoor_space', 'baseball', 'exposed_brick', 'playground', 'swimming_pool', 'two_master_suites', 'efficient', 'theater_room', 'beautiful_backyard', 'coffer_ceiling', 'rv_parking', 'modern_kitchen', 'family_room', 'community_security_features', 'private_bathroom', 'river_access', 'view', 'community_golf', 'cathedral_ceiling', 'disability_features', 'new_roof', 'carport', 'updated_kitchen', 'ensuite', 'well_water', 'boat_dock', 'city_view', 'floor_plan', 'gourmet_kitchen', 'open_floor_plan', 'community_elevator', 'greenhouse', 'media_room', 'fruit_trees', 'central_heat', 'washer_dryer', 'hoa', 'two_or_more_stories', 'basketball_court'}\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:51:18.248693Z",
     "start_time": "2024-05-21T00:51:18.245642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print types of each column\n",
    "print(X_train.dtypes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status                    int64\n",
      "data.total                int64\n",
      "data.count                int64\n",
      "primary_photo            object\n",
      "last_update_date         object\n",
      "source                   object\n",
      "tags                     object\n",
      "permalink                object\n",
      "status                   object\n",
      "list_date                object\n",
      "open_houses             float64\n",
      "description              object\n",
      "branding                 object\n",
      "lead_attributes          object\n",
      "property_id              object\n",
      "photos                   object\n",
      "flags                    object\n",
      "community                object\n",
      "products                 object\n",
      "virtual_tours            object\n",
      "other_listings           object\n",
      "listing_id               object\n",
      "price_reduced_amount    float64\n",
      "matterport               object\n",
      "0                       float64\n",
      "street_view_url          object\n",
      "county                   object\n",
      "0                       float64\n",
      "postal_code              object\n",
      "state                    object\n",
      "coordinate               object\n",
      "city                     object\n",
      "state_code               object\n",
      "line                     object\n",
      "0                       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:51:26.870339Z",
     "start_time": "2024-05-21T00:51:26.868082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if a 'sale_price' column exists and if it contains NaN values\n",
    "if 'sale_price' in X_train.columns:\n",
    "    print(f\"Number of rows before drop: {len(X_train)}\")\n",
    "    X_train = X_train.dropna(subset=['sale_price'])\n",
    "    print(f\"Number of rows after drop: {len(X_train)}\")"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:51:33.007106Z",
     "start_time": "2024-05-21T00:51:33.004455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if a 'property_type' column exists and if it contains NaN values\n",
    "if 'property_type' in X_train.columns:\n",
    "    property_type_nan_count = X_train['property_type'].isna().sum()\n",
    "    print(f\"Number of rows without property_type: {property_type_nan_count}\")"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:51:38.086878Z",
     "start_time": "2024-05-21T00:51:38.072925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count the number of None values per column\n",
    "none_count = X_train.isnull().sum()\n",
    "print(none_count)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status                     0\n",
      "data.total                 0\n",
      "data.count                 0\n",
      "primary_photo            638\n",
      "last_update_date          50\n",
      "source                   350\n",
      "tags                     442\n",
      "permalink                 24\n",
      "status                    24\n",
      "list_date                350\n",
      "open_houses             6552\n",
      "description               24\n",
      "branding                  24\n",
      "lead_attributes           24\n",
      "property_id               24\n",
      "photos                   638\n",
      "flags                     24\n",
      "community               6547\n",
      "products                 411\n",
      "virtual_tours           5457\n",
      "other_listings           262\n",
      "listing_id               350\n",
      "price_reduced_amount    4564\n",
      "matterport                24\n",
      "0                       6552\n",
      "street_view_url           24\n",
      "county                    32\n",
      "0                       6552\n",
      "postal_code               24\n",
      "state                     24\n",
      "coordinate               210\n",
      "city                      28\n",
      "state_code                24\n",
      "line                      37\n",
      "0                       6552\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and concatenate data here\n",
    "# drop or replace values as necessary"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:27:39.850598Z",
     "start_time": "2024-05-21T00:27:39.325630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "\n",
    "data_folder = '/Users/jorgen/Documents/LHL/Supervised-Learning-main/data'\n",
    "json_files = [pos_json for pos_json in os.listdir(data_folder) if pos_json.endswith('.json')]\n",
    "\n",
    "data_frames = []  # Collate all the DataFrames here\n",
    "\n",
    "# Load each JSON file and convert it to pandas DataFrame\n",
    "for file in json_files:\n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        if isinstance(json_data, dict):\n",
    "            # Flatten nested dictionary structures if 'data' is a dictionary\n",
    "            json_df = json_normalize(json_data)\n",
    "        else:\n",
    "            # 'data' is not a dict, so it should be a list or scalar\n",
    "            if isinstance(json_data, list):\n",
    "                # Flatten list of dictionaries if 'data' is a list\n",
    "                json_df = json_normalize(json_data)\n",
    "            else:\n",
    "                # 'data' is a scalar, so convert it into a DataFrame\n",
    "                json_df = pd.DataFrame(data=[json_data], columns=['data'])\n",
    "        data_frames.append(json_df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "print(df.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   status  data.total  data.count  \\\n",
      "0     200         648          42   \n",
      "1     200          48          42   \n",
      "2     200          21          21   \n",
      "3     200          18          16   \n",
      "4     200         442          42   \n",
      "\n",
      "                                        data.results  \n",
      "0  [{'primary_photo': {'href': 'https://ap.rdcpix...  \n",
      "1  [{'primary_photo': {'href': 'https://ap.rdcpix...  \n",
      "2  [{'primary_photo': {'href': 'https://ap.rdcpix...  \n",
      "3  [{'primary_photo': {'href': 'https://ap.rdcpix...  \n",
      "4  [{'primary_photo': {'href': 'https://ap.rdcpix...  \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:28:03.632896Z",
     "start_time": "2024-05-21T00:28:02.866854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 'Explode' the column of lists into separate rows\n",
    "exploded_df = df.explode('data.results')\n",
    "\n",
    "# Now, 'data.results' is a single dictionary per row,\n",
    "# so we can convert these dictionaries into separate columns\n",
    "results_df = exploded_df['data.results'].apply(pd.Series)\n",
    "\n",
    "# Concatenate the original DataFrame with the new 'results' DataFrame\n",
    "flattened_df = pd.concat([exploded_df, results_df], axis=1)\n",
    "\n",
    "# We don't need 'data.results' anymore as its content is now in separate columns\n",
    "flattened_df = flattened_df.drop(columns=['data.results'])\n",
    "\n",
    "# Show the first few rows of the new DataFrame\n",
    "print(flattened_df.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  status  data.total  data.count  \\\n",
      "0    200         648          42   \n",
      "0    200         648          42   \n",
      "0    200         648          42   \n",
      "0    200         648          42   \n",
      "0    200         648          42   \n",
      "\n",
      "                                       primary_photo      last_update_date  \\\n",
      "0  {'href': 'https://ap.rdcpix.com/8c0b391ae4f9bb...  2024-01-17T00:01:55Z   \n",
      "0  {'href': 'https://ap.rdcpix.com/40c55dc5d4e29a...  2024-01-16T16:49:36Z   \n",
      "0  {'href': 'https://ap.rdcpix.com/497a833c1c423b...  2024-01-13T00:03:39Z   \n",
      "0  {'href': 'https://ap.rdcpix.com/ade15664a2e839...  2024-01-16T07:48:30Z   \n",
      "0  {'href': 'https://ap.rdcpix.com/a5dada323a7e61...  2024-01-13T00:01:45Z   \n",
      "\n",
      "                                              source  \\\n",
      "0  {'plan_id': None, 'agents': [{'office_name': N...   \n",
      "0  {'plan_id': None, 'agents': [{'office_name': N...   \n",
      "0  {'plan_id': None, 'agents': [{'office_name': N...   \n",
      "0  {'plan_id': None, 'agents': [{'office_name': N...   \n",
      "0  {'plan_id': None, 'agents': [{'office_name': N...   \n",
      "\n",
      "                                                tags  \\\n",
      "0  [central_air, dishwasher, fireplace, forced_ai...   \n",
      "0  [disability_features, basement, garage_1_or_mo...   \n",
      "0  [carport, central_air, dishwasher, fireplace, ...   \n",
      "0  [central_air, dining_room, disability_features...   \n",
      "0           [dining_room, hardwood_floors, basement]   \n",
      "\n",
      "                                          permalink status  \\\n",
      "0   1869-Pebble-Ct_Harrisburg_PA_17110_M33009-59879   sold   \n",
      "0    3100-N-3rd-St_Harrisburg_PA_17110_M31492-47078   sold   \n",
      "0   7551-Davids-Rd_Harrisburg_PA_17111_M46860-76173   sold   \n",
      "0      2722-Bur-Ct_Harrisburg_PA_17112_M40850-14275   sold   \n",
      "0  404-Crescent-St_Harrisburg_PA_17104_M37433-85520   sold   \n",
      "\n",
      "                     list_date  ...  \\\n",
      "0  2023-11-27T05:15:07.000000Z  ...   \n",
      "0  2023-11-23T11:38:45.000000Z  ...   \n",
      "0  2023-12-16T02:39:18.000000Z  ...   \n",
      "0  2023-10-24T14:15:02.000000Z  ...   \n",
      "0  2023-11-15T17:43:35.000000Z  ...   \n",
      "\n",
      "                                               flags community  \\\n",
      "0  {'is_new_construction': None, 'is_for_rent': N...      None   \n",
      "0  {'is_new_construction': None, 'is_for_rent': N...      None   \n",
      "0  {'is_new_construction': None, 'is_for_rent': N...      None   \n",
      "0  {'is_new_construction': None, 'is_for_rent': N...      None   \n",
      "0  {'is_new_construction': None, 'is_for_rent': N...      None   \n",
      "\n",
      "                       products  virtual_tours  \\\n",
      "0  {'brand_name': 'essentials'}           None   \n",
      "0  {'brand_name': 'essentials'}           None   \n",
      "0  {'brand_name': 'essentials'}           None   \n",
      "0  {'brand_name': 'essentials'}           None   \n",
      "0  {'brand_name': 'essentials'}           None   \n",
      "\n",
      "                                      other_listings  listing_id  \\\n",
      "0  {'rdc': [{'listing_id': '2961973062', 'listing...  2961973062   \n",
      "0  {'rdc': [{'listing_id': '2961936671', 'listing...  2961936671   \n",
      "0  {'rdc': [{'listing_id': '2962590916', 'listing...  2962590916   \n",
      "0  {'rdc': [{'listing_id': '2960823390', 'listing...  2960823390   \n",
      "0  {'rdc': [{'listing_id': '612007585', 'listing_...  2961699518   \n",
      "\n",
      "  price_reduced_amount                                           location  \\\n",
      "0                  NaN  {'address': {'postal_code': '17110', 'state': ...   \n",
      "0                  NaN  {'address': {'postal_code': '17110', 'state': ...   \n",
      "0                  NaN  {'address': {'postal_code': '17111', 'state': ...   \n",
      "0                  NaN  {'address': {'postal_code': '17112', 'state': ...   \n",
      "0                  NaN  {'address': {'postal_code': '17104', 'state': ...   \n",
      "\n",
      "  matterport   0  \n",
      "0      False NaN  \n",
      "0      False NaN  \n",
      "0      False NaN  \n",
      "0      False NaN  \n",
      "0      False NaN  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:28:38.730528Z",
     "start_time": "2024-05-21T00:28:37.409866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Handle the 'location' column\n",
    "location_df = flattened_df['location'].apply(pd.Series)\n",
    "\n",
    "# If 'location' is a nested dictionary, we can flatten it further\n",
    "address_df = location_df['address'].apply(pd.Series)\n",
    "\n",
    "# Now, we concatenate the original DataFrame with the 'location' DataFrame and 'address' DataFrame\n",
    "full_df = pd.concat([flattened_df, location_df, address_df], axis=1)\n",
    "\n",
    "# Drop the original 'location' and 'address' columns\n",
    "full_df = full_df.drop(columns=['location', 'address'])\n",
    "\n",
    "# Show the first few rows\n",
    "print(full_df.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  status  data.total  data.count  \\\n",
      "0    200         648          42   \n",
      "0    200         648          42   \n",
      "0    200         648          42   \n",
      "0    200         648          42   \n",
      "0    200         648          42   \n",
      "\n",
      "                                       primary_photo      last_update_date  \\\n",
      "0  {'href': 'https://ap.rdcpix.com/8c0b391ae4f9bb...  2024-01-17T00:01:55Z   \n",
      "0  {'href': 'https://ap.rdcpix.com/40c55dc5d4e29a...  2024-01-16T16:49:36Z   \n",
      "0  {'href': 'https://ap.rdcpix.com/497a833c1c423b...  2024-01-13T00:03:39Z   \n",
      "0  {'href': 'https://ap.rdcpix.com/ade15664a2e839...  2024-01-16T07:48:30Z   \n",
      "0  {'href': 'https://ap.rdcpix.com/a5dada323a7e61...  2024-01-13T00:01:45Z   \n",
      "\n",
      "                                              source  \\\n",
      "0  {'plan_id': None, 'agents': [{'office_name': N...   \n",
      "0  {'plan_id': None, 'agents': [{'office_name': N...   \n",
      "0  {'plan_id': None, 'agents': [{'office_name': N...   \n",
      "0  {'plan_id': None, 'agents': [{'office_name': N...   \n",
      "0  {'plan_id': None, 'agents': [{'office_name': N...   \n",
      "\n",
      "                                                tags  \\\n",
      "0  [central_air, dishwasher, fireplace, forced_ai...   \n",
      "0  [disability_features, basement, garage_1_or_mo...   \n",
      "0  [carport, central_air, dishwasher, fireplace, ...   \n",
      "0  [central_air, dining_room, disability_features...   \n",
      "0           [dining_room, hardwood_floors, basement]   \n",
      "\n",
      "                                          permalink status  \\\n",
      "0   1869-Pebble-Ct_Harrisburg_PA_17110_M33009-59879   sold   \n",
      "0    3100-N-3rd-St_Harrisburg_PA_17110_M31492-47078   sold   \n",
      "0   7551-Davids-Rd_Harrisburg_PA_17111_M46860-76173   sold   \n",
      "0      2722-Bur-Ct_Harrisburg_PA_17112_M40850-14275   sold   \n",
      "0  404-Crescent-St_Harrisburg_PA_17104_M37433-85520   sold   \n",
      "\n",
      "                     list_date  ...  \\\n",
      "0  2023-11-27T05:15:07.000000Z  ...   \n",
      "0  2023-11-23T11:38:45.000000Z  ...   \n",
      "0  2023-12-16T02:39:18.000000Z  ...   \n",
      "0  2023-10-24T14:15:02.000000Z  ...   \n",
      "0  2023-11-15T17:43:35.000000Z  ...   \n",
      "\n",
      "                                     street_view_url  \\\n",
      "0  https://maps.googleapis.com/maps/api/streetvie...   \n",
      "0  https://maps.googleapis.com/maps/api/streetvie...   \n",
      "0  https://maps.googleapis.com/maps/api/streetvie...   \n",
      "0  https://maps.googleapis.com/maps/api/streetvie...   \n",
      "0  https://maps.googleapis.com/maps/api/streetvie...   \n",
      "\n",
      "                                      county   0  postal_code         state  \\\n",
      "0  {'fips_code': '42043', 'name': 'Dauphin'} NaN        17110  Pennsylvania   \n",
      "0  {'fips_code': '42043', 'name': 'Dauphin'} NaN        17110  Pennsylvania   \n",
      "0  {'fips_code': '42043', 'name': 'Dauphin'} NaN        17111  Pennsylvania   \n",
      "0  {'fips_code': '42043', 'name': 'Dauphin'} NaN        17112  Pennsylvania   \n",
      "0  {'fips_code': '42043', 'name': 'Dauphin'} NaN        17104  Pennsylvania   \n",
      "\n",
      "                              coordinate        city state_code  \\\n",
      "0  {'lon': -76.865096, 'lat': 40.316612}  Harrisburg         PA   \n",
      "0  {'lon': -76.900123, 'lat': 40.296987}  Harrisburg         PA   \n",
      "0  {'lon': -76.757031, 'lat': 40.257449}  Harrisburg         PA   \n",
      "0  {'lon': -76.821034, 'lat': 40.350224}  Harrisburg         PA   \n",
      "0  {'lon': -76.869911, 'lat': 40.259374}  Harrisburg         PA   \n",
      "\n",
      "              line   0  \n",
      "0   1869 Pebble Ct NaN  \n",
      "0    3100 N 3rd St NaN  \n",
      "0   7551 Davids Rd NaN  \n",
      "0      2722 Bur Ct NaN  \n",
      "0  404 Crescent St NaN  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the fact that with tags, there are a lot of categorical variables.\n",
    "- How many columns would we have if we OHE tags, city and state?\n",
    "- Perhaps we can get rid of tags that have a low frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE categorical variables here\n",
    "# tags will have to be done manually"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:29:21.719586Z",
     "start_time": "2024-05-21T00:29:21.712121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_city_cols = full_df['city'].nunique()\n",
    "num_state_cols = full_df['state'].nunique()"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:30:28.429547Z",
     "start_time": "2024-05-21T00:30:28.402823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_tags = set(tag for tags in full_df['tags'] if isinstance(tags, list) for tag in tags)\n",
    "num_tag_cols = len(all_tags)\n",
    "\n",
    "tag_freq = {}\n",
    "for tags_list in full_df['tags']:\n",
    "    if isinstance(tags_list, list):\n",
    "        for tag in tags_list:\n",
    "            if tag not in tag_freq:\n",
    "                tag_freq[tag] = 0\n",
    "            tag_freq[tag] += 1\n",
    "\n",
    "high_freq_tags = {tag for tag, freq in tag_freq.items() if freq > 5}\n",
    "num_high_freq_tags = len(high_freq_tags)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sales will vary drastically between cities and states.  Is there a way to keep information about which city it is without OHE such as using central tendency?\n",
    "- Could we label encode or ordinal encode?  Yes, but this may have undesirable effects, giving nominal data ordinal values.\n",
    "- If you replace cities or states with numerical values, make sure that the data is split so that we don't leak data into the training selection. This is a great time to train test split. Compute on the training data, and join these values to the test data\n",
    "- Drop columns that aren't needed.\n",
    "- Don't keep the list price because it will be too close to the sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train test split here\n",
    "# do something with state and city\n",
    "# drop any other not needed columns"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:31:37.569825Z",
     "start_time": "2024-05-21T00:31:36.474713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Splitting the data into Train and Test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = full_df.drop('list_price', axis=1)  # dropping the 'list_price' column as mentioned\n",
    "y = full_df['list_price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:36:03.329356Z",
     "start_time": "2024-05-21T00:36:03.326970Z"
    }
   },
   "cell_type": "code",
   "source": "print(X_train.columns)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([              'status',           'data.total',           'data.count',\n",
      "              'primary_photo',     'last_update_date',               'source',\n",
      "                       'tags',            'permalink',               'status',\n",
      "                  'list_date',          'open_houses',          'description',\n",
      "                   'branding',      'lead_attributes',          'property_id',\n",
      "                     'photos',                'flags',            'community',\n",
      "                   'products',        'virtual_tours',       'other_listings',\n",
      "                 'listing_id', 'price_reduced_amount',           'matterport',\n",
      "                            0,      'street_view_url',               'county',\n",
      "                            0,          'postal_code',                'state',\n",
      "                 'coordinate',                 'city',           'state_code',\n",
      "                       'line',                      0],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T00:37:36.822168Z",
     "start_time": "2024-05-21T00:37:36.805901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Dealing with 'state' and 'city' columns\n",
    "\n",
    "df_encoded = pd.get_dummies(X_train, columns=['city', 'state'])"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STRETCH**\n",
    "\n",
    "- You're not limited to just using the data provided to you. Think/ do some research about other features that might be useful to predict housing prices. \n",
    "- Can you import and join this data? Make sure you do any necessary preprocessing and make sure it is joined correctly.\n",
    "- Example suggestion: could mortgage interest rates in the year of the listing affect the price? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, join and preprocess new data here"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember all of the EDA that you've been learning about?  Now is a perfect time for it!\n",
    "- Look at distributions of numerical variables to see the shape of the data and detect outliers.\n",
    "- Scatterplots of a numerical variable and the target go a long way to show correlations.\n",
    "- A heatmap will help detect highly correlated features, and we don't want these.\n",
    "- Is there any overlap in any of the features? (redundant information, like number of this or that room...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform EDA here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is a great time to scale the data and save it once it's preprocessed.\n",
    "- You can save it in your data folder, but you may want to make a new `processed/` subfolder to keep it organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
