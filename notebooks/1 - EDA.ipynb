{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The os module has a perfect method to list files in a directory.\n",
    "- Pandas json normalize could work here but is not necessary to convert the JSON data to a dataframe.\n",
    "- You may need a nested for-loop to access each sale!\n",
    "- We've put a lot of time into creating the structure of this repository, and it's a good example for future projects.  In the file functions_variables.py, there is an example function that you can import and use.  If you have any variables, functions or classes that you want to make, they can be put in the functions_variables.py file and imported into a notebook.  Note that only .py files can be imported into a notebook. If you want to import everything from a .py file, you can use the following:\n",
    "```python\n",
    "from functions_variables import *\n",
    "```\n",
    "If you just import functions_variables, then each object from the file will need to be prepended with \"functions_variables\"\\\n",
    "Using this .py file will keep your notebooks very organized and make it easier to reuse code between notebooks."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "from pandas import json_normalize\n",
    "import quandl"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load one file first to see what type of data you're dealing with and what attributes it has"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Specify the directory you want to explore\n",
    "directory = '/Users/jorgen/Documents/LHL/Supervised-Learning-main/data'\n",
    "\n",
    "# Use os.listdir to get files\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Print all files\n",
    "for file in files:\n",
    "    print(file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Path to the JSON file\n",
    "file_path = '/Users/jorgen/Documents/LHL/Supervised-Learning-main/data/AZ_Phoenix_2.json'\n",
    "\n",
    "# Load and inspect the content of the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    pprint(data)  # Pretty print the JSON data to inspect its structure"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, ensure that you have all sales in a dataframe.\n",
    "- Is each cell one value, or do some cells have lists?\n",
    "- Maybe the \"tags\" will help create some features.\n",
    "- What are the data types of each column?\n",
    "- Some sales may not actually include the sale price.  These rows should be dropped.\n",
    "- Some sales don't include the property type.\n",
    "- There are a lot of None values.  Should these be dropped or replaced with something?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Directory containing the JSON files\n",
    "directory = '/Users/jorgen/Documents/LHL/Supervised-Learning-main/data'\n",
    "\n",
    "# List to hold each DataFrame\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.json'):  # Ensure processing only JSON files\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            # Check if the 'sales' key exists in the JSON data\n",
    "            if 'sales' in data:\n",
    "                df = pd.json_normalize(data, record_path=['sales'])\n",
    "            else:\n",
    "                # If 'sales' is not a key, perhaps normalize the entire JSON or another part\n",
    "                df = pd.json_normalize(data)  # Adjust this according to your JSON structure\n",
    "            dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "all_sales_df = pd.concat(dfs, ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reading the CSV data from the specified path into pandas DataFrame `X_train`\n",
    "X_train = pd.read_csv('/Users/jorgen/Documents/LHL/Supervised-Learning-main/dataframe.csv')\n",
    "\n",
    "# Iterating over each column present in the DataFrame\n",
    "for column in X_train.columns:\n",
    "    # Using the `apply` function to go through each cell in the column.\n",
    "    # Apply function takes a lambda function which checks if the cell content is a list.\n",
    "    # `any()` method checks if there is any cell in the column that satisfies the condition.\n",
    "    has_list = X_train[column].apply(lambda x: isinstance(x, list)).any()\n",
    "\n",
    "    # If a list is found in the column (i.e., if `has_list` is `True`),\n",
    "    # then print the column name.\n",
    "    if has_list:\n",
    "        print(f\"Column '{column}' contains a list\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If a 'tags' column exists, try printing out unique set of tags\n",
    "if 'tags' in X_train.columns:\n",
    "    unique_tags = set(tag for tags in X_train['tags'].dropna() for tag in tags)\n",
    "    print(f\"Unique tags: {unique_tags}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print types of each column\n",
    "print(X_train.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if a 'sale_price' column exists and if it contains NaN values\n",
    "if 'sale_price' in X_train.columns:\n",
    "    print(f\"Number of rows before drop: {len(X_train)}\")\n",
    "    X_train = X_train.dropna(subset=['sale_price'])\n",
    "    print(f\"Number of rows after drop: {len(X_train)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if a 'property_type' column exists and if it contains NaN values\n",
    "if 'property_type' in X_train.columns:\n",
    "    property_type_nan_count = X_train['property_type'].isna().sum()\n",
    "    print(f\"Number of rows without property_type: {property_type_nan_count}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Count the number of None values per column\n",
    "none_count = X_train.isnull().sum()\n",
    "print(none_count)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load and concatenate data here\n",
    "# drop or replace values as necessary"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_folder = '/Users/jorgen/Documents/LHL/Supervised-Learning-main/data'\n",
    "json_files = [pos_json for pos_json in os.listdir(data_folder) if pos_json.endswith('.json')]\n",
    "\n",
    "data_frames = []  # Collate all the DataFrames here\n",
    "\n",
    "# Load each JSON file and convert it to pandas DataFrame\n",
    "for file in json_files:\n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        if isinstance(json_data, dict):\n",
    "            # Flatten nested dictionary structures if 'data' is a dictionary\n",
    "            json_df = json_normalize(json_data)\n",
    "        else:\n",
    "            # 'data' is not a dict, so it should be a list or scalar\n",
    "            if isinstance(json_data, list):\n",
    "                # Flatten list of dictionaries if 'data' is a list\n",
    "                json_df = json_normalize(json_data)\n",
    "            else:\n",
    "                # 'data' is a scalar, so convert it into a DataFrame\n",
    "                json_df = pd.DataFrame(data=[json_data], columns=['data'])\n",
    "        data_frames.append(json_df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "print(df.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 'Explode' the column of lists into separate rows\n",
    "exploded_df = df.explode('data.results')\n",
    "\n",
    "# Now, 'data.results' is a single dictionary per row,\n",
    "# so we can convert these dictionaries into separate columns\n",
    "results_df = exploded_df['data.results'].apply(pd.Series)\n",
    "\n",
    "# Concatenate the original DataFrame with the new 'results' DataFrame\n",
    "flattened_df = pd.concat([exploded_df, results_df], axis=1)\n",
    "\n",
    "# We don't need 'data.results' anymore as its content is now in separate columns\n",
    "flattened_df = flattened_df.drop(columns=['data.results'])\n",
    "\n",
    "# Show the first few rows of the new DataFrame\n",
    "print(flattened_df.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Handle the 'location' column\n",
    "location_df = flattened_df['location'].apply(pd.Series)\n",
    "\n",
    "# If 'location' is a nested dictionary, we can flatten it further\n",
    "address_df = location_df['address'].apply(pd.Series)\n",
    "\n",
    "# Now, we concatenate the original DataFrame with the 'location' DataFrame and 'address' DataFrame\n",
    "full_df = pd.concat([flattened_df, location_df, address_df], axis=1)\n",
    "\n",
    "# Drop the original 'location' and 'address' columns\n",
    "full_df = full_df.drop(columns=['location', 'address'])\n",
    "\n",
    "# Show the first few rows\n",
    "print(full_df.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the fact that with tags, there are a lot of categorical variables.\n",
    "- How many columns would we have if we OHE tags, city and state?\n",
    "- Perhaps we can get rid of tags that have a low frequency."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# OHE categorical variables here\n",
    "# tags will have to be done manually"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_city_cols = full_df['city'].nunique()\n",
    "num_state_cols = full_df['state'].nunique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_tags = set(tag for tags in full_df['tags'] if isinstance(tags, list) for tag in tags)\n",
    "num_tag_cols = len(all_tags)\n",
    "\n",
    "tag_freq = {}\n",
    "for tags_list in full_df['tags']:\n",
    "    if isinstance(tags_list, list):\n",
    "        for tag in tags_list:\n",
    "            if tag not in tag_freq:\n",
    "                tag_freq[tag] = 0\n",
    "            tag_freq[tag] += 1\n",
    "\n",
    "high_freq_tags = {tag for tag, freq in tag_freq.items() if freq > 5}\n",
    "num_high_freq_tags = len(high_freq_tags)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sales will vary drastically between cities and states.  Is there a way to keep information about which city it is without OHE such as using central tendency?\n",
    "- Could we label encode or ordinal encode?  Yes, but this may have undesirable effects, giving nominal data ordinal values.\n",
    "- If you replace cities or states with numerical values, make sure that the data is split so that we don't leak data into the training selection. This is a great time to train test split. Compute on the training data, and join these values to the test data\n",
    "- Drop columns that aren't needed.\n",
    "- Don't keep the list price because it will be too close to the sale price."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# perform train test split here\n",
    "# do something with state and city\n",
    "# drop any other not needed columns"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Splitting the data into Train and Test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = full_df.drop('list_price', axis=1)  # dropping the 'list_price' column as mentioned\n",
    "y = full_df['list_price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(X_train.columns)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#X_train",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I did not clean the data properly. For example, in the source, the office_name should be its own column with associated office data. The fips_code should be its own column, and the country name should be its own column. Currently, all that data is mashed into the same column, causing issues."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Adjusting Issues with coordinate column Containing format {'lon': -81.039568, 'lat': 33.983242}\n",
    "\n",
    "# Direct access to 'lon' and 'lat' in dictionary\n",
    "X_train['lon'] = X_train['coordinate'].apply(lambda x: x.get('lon', np.nan) if isinstance(x, dict) else np.nan)\n",
    "X_train['lat'] = X_train['coordinate'].apply(lambda x: x.get('lat', np.nan) if isinstance(x, dict) else np.nan)\n",
    "\n",
    "# Drop the original 'coordinate' column\n",
    "X_train.drop('coordinate', axis=1, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Checking if it worked, Took a while but that worked\n",
    "\n",
    "#X_train"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Fixing County Column to Separate Relevant data into new columns\n",
    "\n",
    "# Direct access to 'fips_code' and 'name' in the dictionary\n",
    "X_train['fips_code'] = X_train['county'].apply(lambda x: x.get('fips_code', np.nan) if isinstance(x, dict) else np.nan)\n",
    "X_train['county'] = X_train['county'].apply(lambda x: x.get('name', np.nan) if isinstance(x, dict) else np.nan)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Checking if it worked\n",
    "#X_train"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Cleaning the other_listing column to Separate data\n",
    "\n",
    "# Define function to extract a key's value from a list of dictionaries\n",
    "def extract_value(list_of_dicts, key):\n",
    "    return [dic.get(key, np.nan) for dic in list_of_dicts if isinstance(dic, dict)]\n",
    "\n",
    "\n",
    "# Start by cleaning 'other_listings' column\n",
    "X_train['other_listings'] = X_train['other_listings'].apply(lambda x: x.get('rdc', []) if isinstance(x, dict) else [])\n",
    "\n",
    "# Create new columns by extracting fields from each dictionary in the 'rdc' list\n",
    "X_train['other_listing_key'] = X_train['other_listings'].apply(lambda x: extract_value(x, 'listing_key'))\n",
    "X_train['other_listing_status'] = X_train['other_listings'].apply(lambda x: extract_value(x, 'status'))\n",
    "X_train['other_listing_primary'] = X_train['other_listings'].apply(lambda x: extract_value(x, 'primary'))\n",
    "\n",
    "# Check the lengths of dataframes\n",
    "print(len(X_train))\n",
    "print(len(X_train['other_listings']))\n",
    "print(len(X_train['other_listing_key']))\n",
    "print(len(X_train['other_listing_status']))\n",
    "print(len(X_train['other_listing_primary']))\n",
    "\n",
    "# Convert lists to string in new columns using apply\n",
    "X_train['other_listing_key'] = X_train['other_listing_key'].apply(lambda x: str(x))\n",
    "X_train['other_listing_status'] = X_train['other_listing_status'].apply(lambda x: ', '.join(str(i) for i in x))\n",
    "X_train['other_listing_primary'] = X_train['other_listing_primary'].apply(lambda x: str(x))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#X_train",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Changing null values to 0 for price reduced amount. Not every house would have reduced their prices.\n",
    "\n",
    "X_train['price_reduced_amount'] = X_train['price_reduced_amount'].fillna(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STRETCH**\n",
    "\n",
    "- You're not limited to just using the data provided to you. Think/ do some research about other features that might be useful to predict housing prices. \n",
    "- Can you import and join this data? Make sure you do any necessary preprocessing and make sure it is joined correctly.\n",
    "- Example suggestion: could mortgage interest rates in the year of the listing affect the price? "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# import, join and preprocess new data here. Plan is to import interest rate data and build model around what housing factors lead to an increase in interest rates & what how interest rate data leads to an increase in housing prices ",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert 'list_date' in X_train to datetime, ignoring errors and stripping time\n",
    "X_train['list_date'] = pd.to_datetime(X_train['list_date'], errors='coerce').dt.date\n",
    "\n",
    "# Now, if there are any None values, they will become NaT in the datetime column\n",
    "# which you can handle according to your project needs (e.g., fill with a placeholder or leave as is)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check how many null values are there in 'list_date'\n",
    "null_dates_count = X_train['list_date'].isnull().sum()\n",
    "print(f\"Number of null dates in X_train: {null_dates_count}\")\n",
    "\n",
    "# Optionally, examine rows with null dates to decide further actions\n",
    "null_dates_rows = X_train[X_train['list_date'].isnull()]\n",
    "print(null_dates_rows)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set your Quandl API key\n",
    "quandl.ApiConfig.api_key = \"evXR317_odmBCAMTi6ym\"\n",
    "\n",
    "# Fetch mortgage rate data from the Quandl API\n",
    "mortgage_data = quandl.get(\"FMAC/30US\", start_date=\"2001-01-01\", end_date=\"2024-12-31\")\n",
    "mortgage_data.reset_index(inplace=True)\n",
    "mortgage_data.rename(columns={'Date': 'Date_API', 'Value': 'Mortgage_Rate'}, inplace=True)\n",
    "\n",
    "# Convert 'Date_API' to proper datetime format\n",
    "mortgage_data['Date_API'] = pd.to_datetime(mortgage_data['Date_API'])\n",
    "\n",
    "# Assuming 'list_date' in X_train is already loaded and needs to be in datetime format\n",
    "X_train['list_date'] = pd.to_datetime(X_train['list_date'])\n",
    "\n",
    "# Drop rows where 'list_date' is null\n",
    "X_train.dropna(subset=['list_date'], inplace=True)\n",
    "\n",
    "# Sort dataframes by the date columns to use merge_asof\n",
    "X_train_sorted = X_train.sort_values('list_date')\n",
    "mortgage_data_sorted = mortgage_data.sort_values('Date_API')\n",
    "\n",
    "# Perform an as-of merge to align dates as closely as possible\n",
    "X_train_merged = pd.merge_asof(X_train_sorted, mortgage_data_sorted, left_on='list_date', right_on='Date_API', direction='nearest')\n",
    "\n",
    "# Drop the 'Date_API' column if it's no longer needed after the merge\n",
    "X_train_merged.drop('Date_API', axis=1, inplace=True)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame to confirm changes\n",
    "X_train_merged.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(X_train.columns)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Remember all of the EDA that you've been learning about?  Now is a perfect time for it!\n",
    "- Look at distributions of numerical variables to see the shape of the data and detect outliers.\n",
    "- Scatterplots of a numerical variable and the target go a long way to show correlations.\n",
    "- A heatmap will help detect highly correlated features, and we don't want these.\n",
    "- Is there any overlap in any of the features? (redundant information, like number of this or that room...)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# perform EDA here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Statistics summary\n",
    "X_train.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Histograms for each numerical variable\n",
    "X_train.hist(bins=50, figsize=(20, 15))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#identifying column names\n",
    "print(X_train.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#first square bracket means I want to create condition, second square bracket indicates exact conditions I want ot see\n",
    "\n",
    "X_train_merged[['Mortgage_Rate', 'price_reduced_amount']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(type(X_train['city'].iloc[0]))\n",
    "print(type(X_train['price_reduced_amount'].iloc[0]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(X_train['county'].isnull().values.any())\n",
    "print(X_train['price_reduced_amount'].isnull().values.any())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is a great time to scale the data and save it once it's preprocessed.\n",
    "- You can save it in your data folder, but you may want to make a new `processed/` subfolder to keep it organized"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Specify the path \n",
    "file_path = \"/Users/jorgen/Documents/LHL/Supervised-Learning-main/processed/processed.csv\"\n",
    "\n",
    "# Check if the directory exists, and create it if it does not\n",
    "directory = \"/Users/jorgen/Documents/LHL/Supervised-Learning-main/processed\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Save the merged DataFrame to a CSV file at the specified path\n",
    "X_train_merged.to_csv(file_path, index=False)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
